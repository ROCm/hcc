
////////////////////////////////////////////////////////////
/// HSAIL builtin functions
////////////////////////////////////////////////////////////

/// get wavefront size
prog function &__wavesize(arg_u32 %dest)()
{
  st_arg_u32 WAVESIZE, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 5.6 : Individual Bit Instructions
////////////////////////////////////////////////////////////

/// popcount_u32_b32
prog function &__popcount_u32_b32(arg_u32 %dest)(arg_u32 %src)
{
  ld_arg_u32 $s0, [%src];
  popcount_u32_b32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// popcount_u32_b64
prog function &__popcount_u32_b64(arg_u32 %dest)(arg_u64 %src)
{
  ld_arg_u64 $d0, [%src];
  popcount_u32_b64 $s0, $d0;
  st_arg_u32 $s0, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 5.7 : Bit String Instructions
////////////////////////////////////////////////////////////

////////////////////
/// bitextract
////////////////////

/// bitextract_u32
prog function &__bitextract_u32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  bitextract_u32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// bitextract_u64
prog function &__bitextract_u64(arg_u64 %dest)(arg_u64 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  bitextract_u64 $d1, $d0, $s0, $s1;
  st_arg_u64 $d1, [%dest];
  ret;
};

/// bitextract_s32
prog function &__bitextract_s32(arg_s32 %dest)(arg_s32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  bitextract_s32 $s3, $s0, $s1, $s2;
  st_arg_s32 $s3, [%dest];
  ret;
};

/// bitextract_s64
prog function &__bitextract_s64(arg_s64 %dest)(arg_s64 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  bitextract_s64 $d1, $d0, $s0, $s1;
  st_arg_s64 $d1, [%dest];
  ret;
};

////////////////////
/// bitinsert
////////////////////

/// bitinsert_u32
prog function &__bitinsert_u32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2, arg_u32 %src3)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  ld_arg_u32 $s3, [%src3];
  bitinsert_u32 $s4, $s0, $s1, $s2, $s3;
  st_arg_u32 $s4, [%dest];
  ret;
};

/// bitinsert_u64
prog function &__bitinsert_u64(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1, arg_u32 %src2, arg_u32 %src3)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  ld_arg_u32 $s0, [%src2];
  ld_arg_u32 $s1, [%src3];
  bitinsert_u64 $d2, $d0, $d1, $s0, $s1;
  st_arg_u64 $d2, [%dest];
  ret;
};

/// bitinsert_s32
prog function &__bitinsert_s32(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1, arg_u32 %src2, arg_u32 %src3)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  ld_arg_u32 $s3, [%src3];
  bitinsert_u32 $s4, $s0, $s1, $s2, $s3;
  st_arg_s32 $s4, [%dest];
  ret;
};

/// bitinsert_s64
prog function &__bitinsert_s64(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1, arg_u32 %src2, arg_u32 %src3)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  ld_arg_u32 $s0, [%src2];
  ld_arg_u32 $s1, [%src3];
  bitinsert_s64 $d2, $d0, $d1, $s0, $s1;
  st_arg_s64 $d2, [%dest];
  ret;
};

////////////////////
/// bitmask
////////////////////

/// bitmask_b32
prog function &__bitmask_b32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  bitmask_b32 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

/// bitmask_b64
prog function &__bitmask_b64(arg_u64 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  bitmask_b64 $d0, $s0, $s1;
  st_arg_u64 $d0, [%dest];
  ret;
};

////////////////////
/// bitrev
////////////////////

/// bitrev_b32
prog function &__bitrev_b32(arg_u32 %dest)(arg_u32 %src0)
{
  ld_arg_u32 $s0, [%src0];
  bitrev_b32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// bitrev_b64
prog function &__bitrev_b64(arg_u64 %dest)(arg_u64 %src0)
{
  ld_arg_u64 $d0, [%src0];
  bitrev_b64 $d1, $d0;
  st_arg_u64 $d1, [%dest];
  ret;
};

////////////////////
/// bitselect
////////////////////

/// bitselect_b32
prog function &__bitselect_b32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  bitselect_b32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// bitselect_b64
prog function &__bitselect_b64(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1, arg_u64 %src2)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  ld_arg_u64 $d2, [%src2];
  bitselect_b64 $d3, $d0, $d1, $d2;
  st_arg_u64 $d3, [%dest];
  ret;
};

////////////////////
/// firstbit
////////////////////

/// firstbit_u32_u32
prog function &__firstbit_u32_u32(arg_u32 %dest)(arg_u32 %src0)
{
  ld_arg_u32 $s0, [%src0];
  firstbit_u32_u32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// firstbit_u32_u64
prog function &__firstbit_u32_u64(arg_u32 %dest)(arg_u64 %src0)
{
  ld_arg_u64 $d0, [%src0];
  firstbit_u32_u64 $s0, $d0;
  st_arg_u32 $s0, [%dest];
  ret;
};

/// firstbit_u32_s32
prog function &__firstbit_u32_s32(arg_u32 %dest)(arg_u32 %src0)
{
  ld_arg_s32 $s0, [%src0];
  firstbit_u32_s32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// firstbit_u32_s64
prog function &__firstbit_u32_s64(arg_u32 %dest)(arg_u64 %src0)
{
  ld_arg_s64 $d0, [%src0];
  firstbit_u32_s64 $s0, $d0;
  st_arg_u32 $s0, [%dest];
  ret;
};

////////////////////
/// lastbit
////////////////////

/// lastbit_u32_u32
prog function &__lastbit_u32_u32(arg_u32 %dest)(arg_u32 %src0)
{
  ld_arg_u32 $s0, [%src0];
  lastbit_u32_u32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// lastbit_u32_u64
prog function &__lastbit_u32_u64(arg_u32 %dest)(arg_u64 %src0)
{
  ld_arg_u64 $d0, [%src0];
  lastbit_u32_u64 $s0, $d0;
  st_arg_u32 $s0, [%dest];
  ret;
};

/// lastbit_u32_s32
prog function &__lastbit_u32_s32(arg_u32 %dest)(arg_u32 %src0)
{
  ld_arg_s32 $s0, [%src0];
  lastbit_u32_s32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// lastbit_u32_s64
prog function &__lastbit_u32_s64(arg_u32 %dest)(arg_u64 %src0)
{
  ld_arg_s64 $d0, [%src0];
  lastbit_u32_s64 $s0, $d0;
  st_arg_u32 $s0, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 5.9 : Packed Data Instructions
////////////////////////////////////////////////////////////

////////////////////
/// shuffle
////////////////////

/// NOTE: shuffle can not be implemented as of now because src2 operand must be a constant value
/// and it's not possible to do so as a library function.
/// shuffle_u8x4
/// shuffle_u8x8
/// shuffle_u16x2
/// shuffle_u16x4
/// shuffle_u32x2
/// shuffle_s8x4
/// shuffle_s8x8
/// shuffle_s16x2
/// shuffle_s16x4
/// shuffle_s32x2
/// shuffle_f8x4
/// shuffle_f8x8
/// shuffle_f16x2
/// shuffle_f16x4
/// shuffle_f32x2

////////////////////
/// unpacklo
////////////////////

// unpacklo_u8x4
prog function &__unpacklo_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpacklo_u8x4 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpacklo_u8x8
prog function &__unpacklo_u8x8(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpacklo_u8x8 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpacklo_u16x2
prog function &__unpacklo_u16x2(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpacklo_u16x2 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpacklo_u16x4
prog function &__unpacklo_u16x4(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpacklo_u16x4 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpacklo_u32x2
prog function &__unpacklo_u32x2(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpacklo_u32x2 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpacklo_s8x4
prog function &__unpacklo_s8x4(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  unpacklo_s8x4 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpacklo_s8x8
prog function &__unpacklo_s8x8(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpacklo_s8x8 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

// unpacklo_s16x2
prog function &__unpacklo_s16x2(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  unpacklo_s16x2 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpacklo_s16x4
prog function &__unpacklo_s16x4(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpacklo_s16x4 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

// unpacklo_s32x2
prog function &__unpacklo_s32x2(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpacklo_s32x2 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

////////////////////
/// unpackhi
////////////////////

// unpackhi_u8x4
prog function &__unpackhi_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpackhi_u8x4 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpackhi_u8x8
prog function &__unpackhi_u8x8(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpackhi_u8x8 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpackhi_u16x2
prog function &__unpackhi_u16x2(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpackhi_u16x2 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpackhi_u16x4
prog function &__unpackhi_u16x4(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpackhi_u16x4 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpackhi_u32x2
prog function &__unpackhi_u32x2(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpackhi_u32x2 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpackhi_s8x4
prog function &__unpackhi_s8x4(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  unpackhi_s8x4 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpackhi_s8x8
prog function &__unpackhi_s8x8(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpackhi_s8x8 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

// unpackhi_s16x2
prog function &__unpackhi_s16x2(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  unpackhi_s16x2 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpackhi_s16x4
prog function &__unpackhi_s16x4(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpackhi_s16x4 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

// unpackhi_s32x2
prog function &__unpackhi_s32x2(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpackhi_s32x2 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

////////////////////
/// pack
////////////////////

// pack_u8x4_u32
prog function &__pack_u8x4_u32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  pack_u8x4_u32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

// pack_u8x8_u32
prog function &__pack_u8x8_u32(arg_u64 %dest)(arg_u64 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_u8x8_u32 $d1, $d0, $s0, $s1;
  st_arg_u64 $d1, [%dest];
  ret;
};

// pack_u8x16_u32 is not implemented as of now

// pack_u16x2_u32
prog function &__pack_u16x2_u32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  pack_u16x2_u32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

// pack_u16x4_u32
prog function &__pack_u16x4_u32(arg_u64 %dest)(arg_u64 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_u16x4_u32 $d1, $d0, $s0, $s1;
  st_arg_u64 $d1, [%dest];
  ret;
};

// pack_u16x8_u32 is not implemented as of now

// pack_u32x2_u32
prog function &__pack_u32x2_u32(arg_u64 %dest)(arg_u64 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_u32x2_u32 $d1, $d0, $s0, $s1;
  st_arg_u64 $d1, [%dest];
  ret;
};

// pack_u32x4_u32 is not implemented as of now

// pack_u64x2_u32 is not implemented as of now

// pack_s8x4_s32
prog function &__pack_s8x4_s32(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1, arg_u32 %src2)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  pack_s8x4_s32 $s3, $s0, $s1, $s2;
  st_arg_s32 $s3, [%dest];
  ret;
};

// pack_s8x8_s32
prog function &__pack_s8x8_s32(arg_s64 %dest)(arg_s64 %src0, arg_s32 %src1, arg_u32 %src2)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_s8x8_s32 $d1, $d0, $s0, $s1;
  st_arg_s64 $d1, [%dest];
  ret;
};

// pack_s8x16_s32 is not implemented as of now

// pack_s16x2_s32
prog function &__pack_s16x2_s32(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1, arg_u32 %src2)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  pack_s16x2_s32 $s3, $s0, $s1, $s2;
  st_arg_s32 $s3, [%dest];
  ret;
};

// pack_s16x4_s32
prog function &__pack_s16x4_s32(arg_u64 %dest)(arg_s64 %src0, arg_s32 %src1, arg_u32 %src2)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_s16x4_s32 $d1, $d0, $s0, $s1;
  st_arg_s64 $d1, [%dest];
  ret;
};

// pack_s16x8_s32 is not implemented as of now

// pack_s32x2_s32
prog function &__pack_s32x2_s32(arg_s64 %dest)(arg_s64 %src0, arg_s32 %src1, arg_u32 %src2)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_s32x2_s32 $d1, $d0, $s0, $s1;
  st_arg_s64 $d1, [%dest];
  ret;
};

// pack_s32x4_s32 is not implemented as of now

// pack_s64x2_s32 is not implemented as of now

// pack_f32x2_f32
prog function &__pack_f32x2_f32(arg_f64 %dest)(arg_f64 %src0, arg_f32 %src1, arg_u32 %src2)
{
  ld_arg_f64 $d0, [%src0];
  ld_arg_f32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_f32x2_f32 $d1, $d0, $s0, $s1;
  st_arg_f64 $d1, [%dest];
  ret;
};

// pack_f32x4_f32 is not implemented as of now

// pack_f64x2_f32 is not implemented as of now

// pack_f16x2_f16 is not implemented as of now

// pack_f16x4_f16 is not implemented as of now

// pack_f16x8_f16 is not implemented as of now

////////////////////
/// unpack
////////////////////

// unpack_u32_u8x4
prog function &__unpack_u32_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpack_u32_u8x4 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpack_u32_u8x8
prog function &__unpack_u32_u8x8(arg_u32 %dest)(arg_u64 %src0, arg_u32 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_u32_u8x8 $s1, $d0, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

// unpack_u32_u8x16 is not implemented as of now

// unpack_u32_u16x2
prog function &__unpack_u32_u16x2(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpack_u32_u16x2 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpack_u32_u16x4
prog function &__unpack_u32_u16x4(arg_u32 %dest)(arg_u64 %src0, arg_u32 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_u32_u16x4 $s1, $d0, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

// unpack_u32_u16x8 is not implemented as of now

// unpack_u32_u32x2
prog function &__unpack_u32_u32x2(arg_u32 %dest)(arg_u64 %src0, arg_u32 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_u32_u32x2 $s1, $d0, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

// unpack_u32_u32x4 is not implemented as of now

// unpack_u32_u64x2 is not implemented as of now

// unpack_s32_s8x4
prog function &__unpack_s32_s8x4(arg_s32 %dest)(arg_s32 %src0, arg_u32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpack_s32_s8x4 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpack_s32_s8x8
prog function &__unpack_s32_s8x8(arg_s32 %dest)(arg_s64 %src0, arg_u32 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_s32_s8x8 $s1, $d0, $s0;
  st_arg_s32 $s1, [%dest];
  ret;
};

// unpack_s32_s8x16 is not implemented as of now

// unpack_s32_s16x2
prog function &__unpack_s32_s16x2(arg_s32 %dest)(arg_s32 %src0, arg_u32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpack_s32_s16x2 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpack_s32_s16x4
prog function &__unpack_s32_s16x4(arg_s32 %dest)(arg_s64 %src0, arg_u32 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_s32_s16x4 $s1, $d0, $s0;
  st_arg_s32 $s1, [%dest];
  ret;
};

// unpack_s32_s16x8 is not implemented as of now

// unpack_s32_s32x2
prog function &__unpack_s32_s32x2(arg_s32 %dest)(arg_s64 %src0, arg_u32 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_s32_s32x2 $s1, $d0, $s0;
  st_arg_s32 $s1, [%dest];
  ret;
};

// unpack_s32_s32x4 is not implemented as of now

// unpack_s32_s64x2 is not implemented as of now

// unpack_f32_f32x2
prog function &__unpack_f32_f32x2(arg_f32 %dest)(arg_f64 %src0, arg_u32 %src1)
{
  ld_arg_f64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_f32_f32x2 $s1, $d0, $s0;
  st_arg_f32 $s1, [%dest];
  ret;
};

// unpack_f32_f32x4 is not implemented as of now

// unpack_f32_f64x2 is not implemented as of now

// unpack_f16_f16x2 is not implemented as of now

// unpack_f16_f16x4 is not implemented as of now

// unpack_f16_f16x8 is not implemented as of now

////////////////////////////////////////////////////////////
/// PRM 5.15 : Multimedia Instructions
////////////////////////////////////////////////////////////

/// bitalign_b32
prog function &__bitalign_b32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  bitalign_b32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// bytealign_b32
prog function &__bytealign_b32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  bytealign_b32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// lerp_u8x4
prog function &__lerp_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  lerp_u8x4 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// packcvt_u8x4_f32
prog function &__packcvt_u8x4_f32(arg_u32 %dest)(arg_f32 %src0, arg_f32 %src1, arg_f32 %src2, arg_f32 %src3)
{
  ld_arg_f32 $s0, [%src0];
  ld_arg_f32 $s1, [%src1];
  ld_arg_f32 $s2, [%src2];
  ld_arg_f32 $s3, [%src3];
  packcvt_u8x4_f32 $s4, $s0, $s1, $s2, $s3;
  st_arg_u32 $s4, [%dest];
  ret;
};

/// unpackcvt_f32_u8x4
/// NOTE:
/// - if src1 is larger or equal to 3, than unpackcvt with src1 as 3 would be used
prog function &__unpackcvt_f32_u8x4(arg_f32 %dest)(arg_f32 %src0, arg_u32 %src1)
{
  ld_arg_f32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  cmp_eq_b1_u32 $c0, $s1, 0;
  cbr_b1 $c0, @unpackcvt_0;
  cmp_eq_b1_u32 $c0, $s1, 1;
  cbr_b1 $c0, @unpackcvt_1;
  cmp_eq_b1_u32 $c0, $s1, 2;
  cbr_b1 $c0, @unpackcvt_2;

@unpackcvt_3:
  unpackcvt_f32_u8x4 $s2, $s0, 3;
  br @return;

@unpackcvt_0:
  unpackcvt_f32_u8x4 $s2, $s0, 0;
  br @return;

@unpackcvt_1:
  unpackcvt_f32_u8x4 $s2, $s0, 1;
  br @return;

@unpackcvt_2:
  unpackcvt_f32_u8x4 $s2, $s0, 2;

@return:
  st_arg_f32 $s2, [%dest];
  ret;
};


/// sad_u32_u32
prog function &__sad_u32_u32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  sad_u32_u32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// sad_u32_u16x2
prog function &__sad_u32_u16x2(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  sad_u32_u16x2 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// sad_u32_u8x4
prog function &__sad_u32_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  sad_u32_u8x4 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// sadhi_u16x2_u8x4
prog function &__sadhi_u16x2_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  sadhi_u16x2_u8x4 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 6.6 : Atomic Instructions
////////////////////////////////////////////////////////////

/// atomic_wrapinc (global)
prog function &__atomic_wrapinc_global(arg_u32 %old)(arg_u64 %addr, arg_u32 %val)
{
  ld_arg_u64 $d0, [%addr];
  ld_arg_u32 $s0, [%val];
  atomic_wrapinc_global_scar_system_u32 $s1, [$d0], $s0;
  st_arg_u32 $s1, [%old];
  ret;
};

/// atomic_wrapinc (local)
prog function &__atomic_wrapinc_local(arg_u32 %old)(arg_u32 %addr, arg_u32 %val)
{
  ld_arg_u32 $s0, [%addr];
  ld_arg_u32 $s1, [%val];
  atomic_wrapinc_group_scar_wg_u32 $s2, [$s0], $s1;
  st_arg_u32 $s2, [%old];
  ret;
};

/// atomic_wrapdec (global)
prog function &__atomic_wrapdec_global(arg_u32 %old)(arg_u64 %addr, arg_u32 %val)
{
  ld_arg_u64 $d0, [%addr];
  ld_arg_u32 $s0, [%val];
  atomic_wrapdec_global_scar_system_u32 $s1, [$d0], $s0;
  st_arg_u32 $s1, [%old];
  ret;
};

/// atomic_wrapdec (local)
prog function &__atomic_wrapdec_local(arg_u32 %old)(arg_u32 %addr, arg_u32 %val)
{
  ld_arg_u32 $s0, [%addr];
  ld_arg_u32 $s1, [%val];
  atomic_wrapdec_group_scar_wg_u32 $s2, [$s0], $s1;
  st_arg_u32 $s2, [%old];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 9.4 : Cross-Lane Instructions
////////////////////////////////////////////////////////////

/// activelanecount_width_u32_b1
/// NOTE:
/// - width is not in use as of now
/// - src is of type u32, and will be converted to b1
prog function &__activelanecount_u32_b1(arg_u32 %dest)(arg_u32 %src)
{
  ld_arg_u32 $s0, [%src];
  cmp_ne_b1_u32 $c0, $s0, 0;
  activelanecount_u32_b1 $s0, $c0;
  st_arg_u32 $s0, [%dest];
  ret;
};

/// activelaneid_width_u32
/// NOTE:
/// - width is not in use as of now
prog function &__activelaneid_u32(arg_u32 %dest)()
{
  activelaneid_u32 $s0;
  st_arg_u32 $s0, [%dest];
  ret;
};

/// activelanemask_v4_width_b64_b1
/// NOTE:
/// - width is not in use as of now
/// - only dest0 is returned as of now
/// - dest1, dest2, dest3 are not returned as of now
/// - input is of type u32, and will be converted to b1
prog function &__activelanemask_v4_b64_b1(arg_u64 %dest0)(arg_u32 %src)
{
  ld_arg_u32 $s0, [%src];
  cmp_ne_b1_u32 $c0, $s0, 0;
  activelanemask_v4_b64_b1 ($d0, $d1, $d2, $d3), $c0;
  st_arg_u64 $d0, [%dest0];
  ret;
};

/// activelanepermute_width_b1 is not implemented as of now
/// activelanepermute_width_b128 is not implemented as of now

/// activelanepermute_width_b32
/// NOTE:
/// - width is not in use as of now
/// - useIdentity is of type u32, and will be converted to b1
prog function &__activelanepermute_b32(arg_u32 %dest)(arg_u32 %src, arg_u32 %laneId, arg_u32 %identity, arg_u32 %useIdentity)
{
  ld_arg_u32 $s0, [%src];
  ld_arg_u32 $s1, [%laneId];
  ld_arg_u32 $s2, [%identity];
  ld_arg_u32 $s3, [%useIdentity];
  cmp_ne_b1_u32 $c0, $s3, 0;
  activelanepermute_b32 $s3, $s0, $s1, $s2, $c0;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// activelanepermute_width_b64
/// NOTE:
/// - width is not in use as of now
/// - useIdentity is of type b32, and will be converted to b1
prog function &__activelanepermute_b64(arg_u64 %dest)(arg_u64 %src, arg_u32 %laneId, arg_u64 %identity, arg_u32 %useIdentity)
{
  ld_arg_u64 $d0, [%src];
  ld_arg_u32 $s0, [%laneId];
  ld_arg_u64 $d1, [%identity];
  ld_arg_u32 $s1, [%useIdentity];
  cmp_ne_b1_u32 $c0, $s1, 0;
  activelanepermute_b64 $d2, $d0, $s0, $d1, $c0;
  st_arg_u64 $d2, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 11.4 : Miscellaneous Instructions
////////////////////////////////////////////////////////////

/// get system timestamp
prog function &__clock_u64(arg_u64 %dest)() {
  clock_u64 $d0;
  st_arg_u64 $d0, [%dest];
  ret;
};

/// get hardware cycle count
/// NOTE:
/// - There is no corresponding HSAIL instruction so we always return 0 here
prog function &__cycle_u64(arg_u64 %dest)() {
  st_arg_u64 0, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// Dynamic group segment
////////////////////////////////////////////////////////////

/// global variable to store the size of static group segment
/// the value would be set by Kalmar runtime prior to kernel dispatch
prog global_u32 &hcc_static_group_segment_size = 0;

/// global variable to store the size of dynamic group segment
/// the value would be set by Kalmar runtime prior to kernel dispatch
prog global_u32 &hcc_dynamic_group_segment_size = 0;

/// get_static_group_segment_size : return the size of static group segment
prog function &get_static_group_segment_size(arg_u32 %ret)()
{
  ld_global_u32 $s0, [&hcc_static_group_segment_size];
  st_arg_u32 $s0, [%ret];
  ret;
};

/// get_dynamic_group_segment_size : return the size of dynamic group segment
prog function &get_dynamic_group_segment_size(arg_u32 %ret)()
{
  ld_global_u32 $s0, [&hcc_dynamic_group_segment_size];
  st_arg_u32 $s0, [%ret];
  ret;
};

/// get_group_segment_addr : get arbitrary address location within group segment
///
/// This function would return an arbitrary location within group segment.
/// It takes an u32 offset argument and returns the given address in the
/// group segment.  The base of group segment is fetched with groupbaseptr_u32.
///
prog function &get_group_segment_addr(arg_u32 %ret)(arg_u32 %offset)
{
  ld_arg_u32 $s1, [%offset];

  groupbaseptr_u32 $s0;

  add_u32 $s0, $s0, $s1;

  st_arg_u32 $s0, [%ret];
  ret;
};

/// get_dynamic_group_segment : get the pointer to the beginning of dynamic
/// group segment
///
prog function &get_dynamic_group_segment(arg_u32 %ret)()
{
  // call get_static_group_segment_size
  {
    arg_u32 %res1;
    call &get_static_group_segment_size(%res1)();
    ld_arg_u32 $s0, [%res1];
  }
  // call get_group_segment_addr
  {
    arg_u32 %offset;
    arg_u32 %res2;
    // fill in the argument
    st_arg_u32 $s0, [%offset];
    call &get_group_segment_addr(%res2)(%offset);
    ld_arg_u32 $s1, [%res2];
  }
  st_arg_u32 $s1, [%ret];
  ret;
};

////////////////////////////////////////////////////////////
/// New / delete within kernels
////////////////////////////////////////////////////////////

prog global_u64 &signal_Xmalloc = 0;
prog global_u64 &signal_malloc = 0;
prog global_u64 &ptr_a_address = 0;
prog global_u64 &ptr_b_address = 0;
prog global_u64 &ptr_c_address = 0;
prog global_u64 &ptr_x_address = 0;
prog global_u64 &ptr_y_address = 0;
prog global_u64 &ptr_z_address = 0;

prog function &_Z14putXmallocFlagm()(arg_u64 %arg_p0)
{
        ld_arg_u64  $d0, [%arg_p0];
        st_global_u64 $d0, [&signal_Xmalloc];
        ret;
};

prog function &_Z13putMallocFlagm()(arg_u64 %arg_p0)
{
        ld_arg_u64  $d0, [%arg_p0];
        st_global_u64 $d0, [&signal_malloc];
        ret;
};

prog function &_Z9put_ptr_aPv2()(arg_u64 %arg_p0)
{
        ld_arg_u64  $d0, [%arg_p0];
        st_global_u64 $d0, [&ptr_a_address];
        ret;
};

prog function &_Z9put_ptr_bPv3()(arg_u64 %arg_p0)
{
        ld_arg_u64  $d0, [%arg_p0];
        st_global_u64 $d0, [&ptr_b_address];
        ret;
};

prog function &_Z9put_ptr_cPv4()(arg_u64 %arg_p0)
{
        ld_arg_u64  $d0, [%arg_p0];
        st_global_u64 $d0, [&ptr_c_address];
        ret;
};

prog function &_Z9put_ptr_xPv5()(arg_u64 %arg_p0)
{
        ld_arg_u64  $d0, [%arg_p0];
        st_global_u64 $d0, [&ptr_x_address];
        ret;
};

prog function &_Z9put_ptr_yPv6()(arg_u64 %arg_p0)
{
        ld_arg_u64  $d0, [%arg_p0];
        st_global_u64 $d0, [&ptr_y_address];
        ret;
};

prog function &_Z9put_ptr_zPv7()(arg_u64 %arg_p0)
{
        ld_arg_u64  $d0, [%arg_p0];
        st_global_u64 $d0, [&ptr_z_address];
        ret;
};

/*
  Implementation of Xfree/free, observe the hsail and extract it.  

  parallel_for_each(
    Concurrency::extent<1>(vecSize).tile<tileSize>(),
    [=](Concurrency::tiled_index<tileSize> tidx) restrict(amp) {

    int global = tidx.global[0];
    int local = tidx.local[0];
    int tile = tidx.tile[0];

    // store the parameter
    (ptr_y + global)->store(address, std::memory_order_release);

    // store the signal value
    (ptr_x + global)->store(2, std::memory_order_release);

    // wait until syscall returns
    while ((ptr_x + global)->load(std::memory_order_acquire));
  });

*/

// Xfree/free
prog function &_ZdlPv()(arg_u64 %arg_p0)
{

@ZZ4mainEN3_EC__219__cxxamp_trampolineE_1PNSt3__16atomicIlEElPNS1_IiEE_entry:

ld_global_u64 $d10, [&signal_malloc];
signalnoret_add_screl_s64_sig64 $d10, 1;

// BB#0:
workitemabsid_u32       $s0, 0;
cvt_u64_u32     $d0, $s0;
ld_kernarg_align(8)_width(all)_u64      $d1, [0];
add_u64 $d0, $d0, $d1;
shl_u64 $d0, $d0, 32;
shr_s64 $d1, $d0, 32;
shl_u64 $d0, $d1, 2;
ld_global_u64 $d2, [&ptr_x_address];
add_u64 $d0, $d2, $d0;
shl_u64 $d1, $d1, 3;
ld_global_u64 $d2, [&ptr_y_address];
add_u64 $d1, $d2, $d1;
mov_b32 $s0, 2;
ld_arg_u64 $d2, [%arg_p0];
atomicnoret_st_global_screl_system_b64  [$d1], $d2;
atomicnoret_st_global_screl_system_b32  [$d0], $s0;

@BB0_1:
atomic_ld_global_scacq_system_b32       $s0, [$d0];
cmp_ne_b1_s32   $c0, $s0, 0;
cbr_b1  $c0, @BB0_1;
// BB#2:                                // %_ZZ4mainENK3$_2clE_1N11Concurrency11tiled_indexILi4ELi0ELi0EEE.exit

ld_global_u64 $d10, [&signal_malloc];
signalnoret_sub_screl_s64_sig64 $d10, 1;

ret;

};

prog function &_ZdaPv()(arg_u64 %arg_p0)
{

@ZZ4mainEN3_EC__219__cxxamp_trampolineE_1PNSt3__16atomicIlEElPNS1_IiEE_entry:

ld_global_u64 $d10, [&signal_malloc];
signalnoret_add_screl_s64_sig64 $d10, 1;

// BB#0:
workitemabsid_u32       $s0, 0;
cvt_u64_u32     $d0, $s0;
ld_kernarg_align(8)_width(all)_u64      $d1, [0];
add_u64 $d0, $d0, $d1;
shl_u64 $d0, $d0, 32;
shr_s64 $d1, $d0, 32;
shl_u64 $d0, $d1, 2;
ld_global_u64 $d2, [&ptr_x_address];
add_u64 $d0, $d2, $d0;
shl_u64 $d1, $d1, 3;
ld_global_u64 $d2, [&ptr_y_address];
add_u64 $d1, $d2, $d1;
mov_b32 $s0, 2;
ld_arg_u64 $d2, [%arg_p0];
atomicnoret_st_global_screl_system_b64  [$d1], $d2;
atomicnoret_st_global_screl_system_b32  [$d0], $s0;

@BB0_1:
atomic_ld_global_scacq_system_b32       $s0, [$d0];
cmp_ne_b1_s32   $c0, $s0, 0;
cbr_b1  $c0, @BB0_1;
// BB#2:                                // %_ZZ4mainENK3$_2clE_1N11Concurrency11tiled_indexILi4ELi0ELi0EEE.exit

ld_global_u64 $d10, [&signal_malloc];
signalnoret_sub_screl_s64_sig64 $d10, 1;

ret;

};

/*
  Implementation of malloc, observe the hsail and extract it.

  parallel_for_each(
    Concurrency::extent<1>(vecSize).tile<tileSize>(),
    [=](Concurrency::tiled_index<tileSize> tidx) restrict(amp) {

    int global = tidx.global[0];
    int local = tidx.local[0];
    int tile = tidx.tile[0];

    (ptr_y + global)->store(n, std::memory_order_release);

    (ptr_x + global)->store(1, std::memory_order_release);

    while ((ptr_x + global)->load(std::memory_order_acquire));

    // load result from CPU
    long result = (ptr_y + global)->load(std::memory_order_acquire);

    // test access the memory allocated
    int *p_counter = (int *)result;
    *p_counter = 1;
    int header_offset = sizeof(int);
    int *p_header = (int *)((char *)p_counter + header_offset);
    *p_header = header_offset;
    char *alloc = (char *)(p_header + 1);

    long address = (long)alloc;
 
    // store result
    (ptr_z + global)->store(address, std::memory_order_release);
  });

*/

// malloc version
prog function &_Znwm_malloc(arg_u64 %ret_r0)(arg_u64 %arg_p0)
{

@ZZ4mainEN3_EC__119__cxxamp_trampolineE_0lPNSt3__16atomicIlEEPNS1_IiEES3__entry:

ld_global_u64 $d10, [&signal_malloc];
signalnoret_add_screl_s64_sig64 $d10, 1;

// BB#0:
workitemabsid_u32       $s0, 0;
cvt_u64_u32     $d0, $s0;
ld_kernarg_align(8)_width(all)_u64      $d1, [0];
add_u64 $d0, $d0, $d1;
shl_u64 $d0, $d0, 32;
shr_s64 $d1, $d0, 32;
shl_u64 $d0, $d1, 3;
shl_u64 $d1, $d1, 2;
ld_arg_u64 $d2, [%arg_p0];
shl_u64 $d4, $d2, 32;
ld_global_u64 $d2, [&ptr_y_address];
ld_global_u64 $d3, [&ptr_x_address];
add_u64 $d3, $d3, $d1;
add_u64 $d2, $d2, $d0;
add_u64 $d1, $d4, 34359738368;
shr_s64 $d4, $d1, 32;
ld_global_u64 $d1, [&ptr_z_address];
mov_b32 $s0, 1;
atomicnoret_st_global_screl_system_b64  [$d2], $d4;
atomicnoret_st_global_screl_system_b32  [$d3], $s0;

@BB0_1:
atomic_ld_global_scacq_system_b32       $s1, [$d3];
cmp_ne_b1_s32   $c0, $s1, 0;
cbr_b1  $c0, @BB0_1;
// BB#2:                                // %_ZZ4mainENK3$_1clE_0N11Concurrency11tiled_indexILi1ELi0ELi0EEE.exit
atomic_ld_global_scacq_system_b64       $d2, [$d2];
st_global_align(4)_u32  $s0, [$d2];
mov_b32 $s0, 4;
st_global_align(4)_u32  $s0, [$d2+4];
add_u64 $d0, $d1, $d0;
add_u64 $d1, $d2, 8;
// atomicnoret_st_global_screl_system_b64       [$d0], $d1;
st_arg_u64  $d1, [%ret_r0];

ld_global_u64 $d10, [&signal_malloc];
signalnoret_sub_screl_s64_sig64 $d10, 1;

ret;

};

prog function &_Znam_malloc(arg_u64 %ret_r0)(arg_u64 %arg_p0)
{

@ZZ4mainEN3_EC__119__cxxamp_trampolineE_0lPNSt3__16atomicIlEEPNS1_IiEES3__entry:

ld_global_u64 $d10, [&signal_malloc];
signalnoret_add_screl_s64_sig64 $d10, 1;

// BB#0:
workitemabsid_u32       $s0, 0;
cvt_u64_u32     $d0, $s0;
ld_kernarg_align(8)_width(all)_u64      $d1, [0];
add_u64 $d0, $d0, $d1;
shl_u64 $d0, $d0, 32;
shr_s64 $d1, $d0, 32;
shl_u64 $d0, $d1, 3;
shl_u64 $d1, $d1, 2;
ld_arg_u64 $d2, [%arg_p0];
shl_u64 $d4, $d2, 32;
ld_global_u64 $d2, [&ptr_y_address];
ld_global_u64 $d3, [&ptr_x_address];
add_u64 $d3, $d3, $d1;
add_u64 $d2, $d2, $d0;
add_u64 $d1, $d4, 34359738368;
shr_s64 $d4, $d1, 32;
ld_global_u64 $d1, [&ptr_z_address];
mov_b32 $s0, 1;
atomicnoret_st_global_screl_system_b64  [$d2], $d4;
atomicnoret_st_global_screl_system_b32  [$d3], $s0;

@BB0_1:
atomic_ld_global_scacq_system_b32       $s1, [$d3];
cmp_ne_b1_s32   $c0, $s1, 0;
cbr_b1  $c0, @BB0_1;
// BB#2:                                // %_ZZ4mainENK3$_1clE_0N11Concurrency11tiled_indexILi1ELi0ELi0EEE.exit
atomic_ld_global_scacq_system_b64       $d2, [$d2];
st_global_align(4)_u32  $s0, [$d2];
mov_b32 $s0, 4;
st_global_align(4)_u32  $s0, [$d2+4];
add_u64 $d0, $d1, $d0;
add_u64 $d1, $d2, 8;
// atomicnoret_st_global_screl_system_b64       [$d0], $d1;
st_arg_u64  $d1, [%ret_r0];

ld_global_u64 $d10, [&signal_malloc];
signalnoret_sub_screl_s64_sig64 $d10, 1;

ret;

};

/*
  Implementation of Xmalloc, observe the hsail and extract it.

  parallel_for_each(
    Concurrency::extent<1>(vecSize).tile<tileSize>(),
    [=](Concurrency::tiled_index<tileSize> tidx) restrict(amp) {

    #define MAX_TILE_SIZE 256

    int global = tidx.global[0];
    int local = tidx.local[0];
    int tile = tidx.tile[0];

    tile_static long g_idata[MAX_TILE_SIZE * 2]; // g_idata only
    g_idata[local] = n; // g_idata only

    tile_static long temp[MAX_TILE_SIZE * 2];
    int offset = 1;

    tidx.barrier.wait();

    temp[2 * local] = g_idata[2 * local];
    temp[2 * local + 1] = g_idata[2 * local + 1];

    for (int d = currentworkgroupsize >> 1; d > 0; d >>= 1) // build
    {
      tidx.barrier.wait();

      if (local < d)
      {
        int ai = offset * (2 * local + 1) - 1;
        int bi = offset * (2 * local + 2) - 1;

        temp[bi] += temp[ai];
      }
      offset *= 2;
    }

    if (local == 0) { temp[currentworkgroupsize - 1] = 0; }

    for (int d = 1; d < currentworkgroupsize; d *= 2)
    {
      offset >>= 1;
      tidx.barrier.wait();

      if (local < d)
      {
        int ai = offset * (2 * local + 1) - 1;
        int bi = offset * (2 * local + 2) - 1;

        long t = temp[ai];
        temp[ai] = temp[bi];
        temp[bi] += t;
      }
    }

    tidx.barrier.wait();

    if (local == 0) {
      long amount = temp[currentworkgroupsize - 1] + g_idata[currentworkgroupsize - 1] + sizeof(int) * currentworkgroupsize + sizeof(int);

      (ptr_b + tile)->store(amount, std::memory_order_release);

      (ptr_a + tile)->store(1, std::memory_order_release);

      while ((ptr_a + tile)->load(std::memory_order_acquire));
    }

    tidx.barrier.wait();

    // load result from CPU
    long result = (ptr_b + tile)->load(std::memory_order_acquire);

    // test access the memory allocated
    int *p_counter = (int *) result;

    if (local == 0) {
      *p_counter = currentworkgroupsize;
    }

    int header_offset = (int)temp[local] + sizeof(int) * local + sizeof(int);
    int *p_header = (int *)((char *)p_counter + header_offset);
    *p_header = header_offset;
    char *alloc = (char *)(p_header + 1);
    long address = (long)alloc;

    // store result
    (ptr_c + global)->store(address, std::memory_order_release);
  });

*/

// Xmalloc version
prog function &_Znwm(arg_u64 %ret_r0)(arg_u64 %arg_p0)
{

align(16) group_u64 %__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12[512];
align(16) group_u64 %__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13[512];

@ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__entry:

ld_global_u64 $d10, [&signal_Xmalloc];
signalnoret_add_screl_s64_sig64 $d10, 1;

// BB#0:
workitemid_u32	$s2, 0;
shl_u32	$s0, $s2, 3;
ld_arg_u64 $d0, [%arg_p0];
st_group_align(8)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s0];
barrier;
shl_u32	$s6, $s2, 1;
or_b32	$s3, $s6, 1;
shl_u32	$s1, $s6, 3;
ld_group_align(16)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s1];
st_group_align(16)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s1];
shl_u32	$s5, $s3, 3;
workitemabsid_u32	$s4, 0;
ld_global_u64 $d0, [&ptr_c_address];
ld_global_u64 $d2, [&ptr_a_address];
ld_global_u64 $d1, [&ptr_b_address];
currentworkgroupsize_u32 $s1, 0;
ld_kernarg_align(8)_width(all)_u64	$d3, [0];
ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s5];
st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s5];
shr_s32	$s7, $s1, 1;
cmp_lt_b1_s32	$c0, $s7, 1;
cbr_b1	$c0, @BB0_1;
// BB#2:                                // %.lr.ph11.i
add_u32	$s8, $s6, 2;
mov_b32	$s5, 1;

@BB0_3:
barrier;
cmp_ge_b1_s32	$c0, $s2, $s7;
cbr_b1	$c0, @BB0_5;
// BB#4:
mul_u32	$s9, $s5, $s3;
shl_u32	$s9, $s9, 3;
ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
mul_u32	$s9, $s5, $s8;
shl_u32	$s9, $s9, 3;
ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
add_u64	$d4, $d5, $d4;
st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];

@BB0_5:
shl_u32	$s5, $s5, 1;
shr_s32	$s7, $s7, 1;
cmp_gt_b1_s32	$c0, $s7, 0;
cbr_b1	$c0, @BB0_3;
br	@BB0_6;

@BB0_1:
mov_b32	$s5, 1;

@BB0_6:
// %._crit_edge12.i
cmp_ne_b1_s32	$c0, $s2, 0;
cbr_b1	$c0, @BB0_8;
// BB#7:
shl_u32	$s7, $s1, 3;
mov_b64	$d4, 0;
st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s7-8];

@BB0_8:
// %.preheader.i
cmp_lt_b1_s32	$c1, $s1, 2;
cbr_b1	$c1, @BB0_13;
// BB#9:                                // %.lr.ph.i
add_u32	$s6, $s6, 2;
mov_b32	$s7, 1;

@BB0_10:
barrier;
shr_s32	$s5, $s5, 1;
cmp_ge_b1_s32	$c1, $s2, $s7;
cbr_b1	$c1, @BB0_12;
// BB#11:
mul_u32	$s8, $s5, $s3;
shl_u32	$s8, $s8, 3;
ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s8-8];
mul_u32	$s9, $s5, $s6;
shl_u32	$s9, $s9, 3;
ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
st_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s8-8];
ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
add_u64	$d4, $d5, $d4;
st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];

@BB0_12:
shl_u32	$s7, $s7, 1;
cmp_lt_b1_s32	$c1, $s7, $s1;
cbr_b1	$c1, @BB0_10;

@BB0_13:
// %._crit_edge.i
cvt_u64_u32	$d4, $s4;
workgroupid_u32	$s3, 0;
add_u64	$d3, $d4, $d3;
pack_u32x2_u32	$d4, u32x2(0,0), $s2, 1;
barrier;
cmp_eq_b1_s32	$c1, $s2, 0;
cbr_b1	$c1, @BB0_15;
// BB#14:                                // %._crit_edge13.i
cvt_s64_s32	$d5, $s3;
br	@BB0_17;

@BB0_15:
add_u32	$s2, $s1, 4294967295;
shl_u32	$s2, $s2, 3;
cvt_s64_s32	$d5, $s1;
shl_u64	$d6, $d5, 2;
ld_group_align(8)_width(WAVESIZE)_u64	$d7, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s2];
cvt_s64_s32	$d5, $s3;
shl_u64	$d8, $d5, 3;
shl_u64	$d9, $d5, 2;
add_u64	$d2, $d2, $d9;
add_u64	$d8, $d1, $d8;
add_u64	$d6, $d6, $d7;
ld_group_align(8)_width(WAVESIZE)_u64	$d7, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s2];
add_u64	$d6, $d6, $d7;
mov_b32	$s2, 1;
add_u64	$d6, $d6, 4;
atomicnoret_st_global_screl_system_b64	[$d8], $d6;
atomicnoret_st_global_screl_system_b32	[$d2], $s2;

@BB0_16:
atomic_ld_global_scacq_system_b32	$s2, [$d2];
cmp_ne_b1_s32	$c1, $s2, 0;
cbr_b1	$c1, @BB0_16;

@BB0_17:
// %.loopexit.i
shl_u64	$d2, $d5, 3;
add_u64	$d1, $d1, $d2;
barrier;
atomic_ld_global_scacq_system_b64	$d1, [$d1];
cbr_b1	$c0, @BB0_19;
// BB#18:
st_global_align(4)_u32	$s1, [$d1];

@BB0_19:
// %_ZZ4mainENK3$_2clE_1N11Concurrency11tiled_indexILi4ELi0ELi0EEE.exit
shr_s64	$d2, $d4, 30;
ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s0];
and_b64	$d4, $d4, 4294967295;
add_u64	$d2, $d2, $d4;
add_u64	$d2, $d2, 4;
cvt_u32_u64	$s0, $d2;
shl_u64	$d4, $d2, 32;
shl_u64	$d2, $d3, 32;
shr_s64	$d2, $d2, 32;
shr_s64	$d3, $d4, 32;
add_u64	$d1, $d1, $d3;
st_global_align(4)_u32	$s0, [$d1];
shl_u64	$d2, $d2, 3;
add_u64	$d0, $d0, $d2;
add_u64	$d1, $d1, 4;
// atomicnoret_st_global_screl_system_b64   [$d0], $d1;
st_arg_u64  $d1, [%ret_r0];

ld_global_u64 $d10, [&signal_Xmalloc];
signalnoret_sub_screl_s64_sig64 $d10, 1;

ret;

};

prog function &_Znam(arg_u64 %ret_r0)(arg_u64 %arg_p0)
{

align(16) group_u64 %__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12[512];
align(16) group_u64 %__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13[512];

@ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__entry:

ld_global_u64 $d10, [&signal_Xmalloc];
signalnoret_add_screl_s64_sig64 $d10, 1;

// BB#0:
workitemid_u32	$s2, 0;
shl_u32	$s0, $s2, 3;
ld_arg_u64 $d0, [%arg_p0];
st_group_align(8)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s0];
barrier;
shl_u32	$s6, $s2, 1;
or_b32	$s3, $s6, 1;
shl_u32	$s1, $s6, 3;
ld_group_align(16)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s1];
st_group_align(16)_u64	$d0, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s1];
shl_u32	$s5, $s3, 3;
workitemabsid_u32	$s4, 0;
ld_global_u64 $d0, [&ptr_c_address];
ld_global_u64 $d2, [&ptr_a_address];
ld_global_u64 $d1, [&ptr_b_address];
currentworkgroupsize_u32 $s1, 0;
ld_kernarg_align(8)_width(all)_u64	$d3, [0];
ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s5];
st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s5];
shr_s32	$s7, $s1, 1;
cmp_lt_b1_s32	$c0, $s7, 1;
cbr_b1	$c0, @BB0_1;
// BB#2:                                // %.lr.ph11.i
add_u32	$s8, $s6, 2;
mov_b32	$s5, 1;

@BB0_3:
barrier;
cmp_ge_b1_s32	$c0, $s2, $s7;
cbr_b1	$c0, @BB0_5;
// BB#4:
mul_u32	$s9, $s5, $s3;
shl_u32	$s9, $s9, 3;
ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
mul_u32	$s9, $s5, $s8;
shl_u32	$s9, $s9, 3;
ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
add_u64	$d4, $d5, $d4;
st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];

@BB0_5:
shl_u32	$s5, $s5, 1;
shr_s32	$s7, $s7, 1;
cmp_gt_b1_s32	$c0, $s7, 0;
cbr_b1	$c0, @BB0_3;
br	@BB0_6;

@BB0_1:
mov_b32	$s5, 1;

@BB0_6:
// %._crit_edge12.i
cmp_ne_b1_s32	$c0, $s2, 0;
cbr_b1	$c0, @BB0_8;
// BB#7:
shl_u32	$s7, $s1, 3;
mov_b64	$d4, 0;
st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s7-8];

@BB0_8:
// %.preheader.i
cmp_lt_b1_s32	$c1, $s1, 2;
cbr_b1	$c1, @BB0_13;
// BB#9:                                // %.lr.ph.i
add_u32	$s6, $s6, 2;
mov_b32	$s7, 1;

@BB0_10:
barrier;
shr_s32	$s5, $s5, 1;
cmp_ge_b1_s32	$c1, $s2, $s7;
cbr_b1	$c1, @BB0_12;
// BB#11:
mul_u32	$s8, $s5, $s3;
shl_u32	$s8, $s8, 3;
ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s8-8];
mul_u32	$s9, $s5, $s6;
shl_u32	$s9, $s9, 3;
ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
st_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s8-8];
ld_group_align(8)_u64	$d5, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];
add_u64	$d4, $d5, $d4;
st_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s9-8];

@BB0_12:
shl_u32	$s7, $s7, 1;
cmp_lt_b1_s32	$c1, $s7, $s1;
cbr_b1	$c1, @BB0_10;

@BB0_13:
// %._crit_edge.i
cvt_u64_u32	$d4, $s4;
workgroupid_u32	$s3, 0;
add_u64	$d3, $d4, $d3;
pack_u32x2_u32	$d4, u32x2(0,0), $s2, 1;
barrier;
cmp_eq_b1_s32	$c1, $s2, 0;
cbr_b1	$c1, @BB0_15;
// BB#14:                                // %._crit_edge13.i
cvt_s64_s32	$d5, $s3;
br	@BB0_17;

@BB0_15:
add_u32	$s2, $s1, 4294967295;
shl_u32	$s2, $s2, 3;
cvt_s64_s32	$d5, $s1;
shl_u64	$d6, $d5, 2;
ld_group_align(8)_width(WAVESIZE)_u64	$d7, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s2];
cvt_s64_s32	$d5, $s3;
shl_u64	$d8, $d5, 3;
shl_u64	$d9, $d5, 2;
add_u64	$d2, $d2, $d9;
add_u64	$d8, $d1, $d8;
add_u64	$d6, $d6, $d7;
ld_group_align(8)_width(WAVESIZE)_u64	$d7, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN7g_idataE_12][$s2];
add_u64	$d6, $d6, $d7;
mov_b32	$s2, 1;
add_u64	$d6, $d6, 4;
atomicnoret_st_global_screl_system_b64	[$d8], $d6;
atomicnoret_st_global_screl_system_b32	[$d2], $s2;

@BB0_16:
atomic_ld_global_scacq_system_b32	$s2, [$d2];
cmp_ne_b1_s32	$c1, $s2, 0;
cbr_b1	$c1, @BB0_16;

@BB0_17:
// %.loopexit.i
shl_u64	$d2, $d5, 3;
add_u64	$d1, $d1, $d2;
barrier;
atomic_ld_global_scacq_system_b64	$d1, [$d1];
cbr_b1	$c0, @BB0_19;
// BB#18:
st_global_align(4)_u32	$s1, [$d1];

@BB0_19:
// %_ZZ4mainENK3$_2clE_1N11Concurrency11tiled_indexILi4ELi0ELi0EEE.exit
shr_s64	$d2, $d4, 30;
ld_group_align(8)_u64	$d4, [%__hsa_replaced_ZZ4mainEN3_EC__219__cxxamp_trampolineE_1liPNSt3__16atomicIlEEPNS1_IiEES3__ZZ4mainEN4tempE_13][$s0];
and_b64	$d4, $d4, 4294967295;
add_u64	$d2, $d2, $d4;
add_u64	$d2, $d2, 4;
cvt_u32_u64	$s0, $d2;
shl_u64	$d4, $d2, 32;
shl_u64	$d2, $d3, 32;
shr_s64	$d2, $d2, 32;
shr_s64	$d3, $d4, 32;
add_u64	$d1, $d1, $d3;
st_global_align(4)_u32	$s0, [$d1];
shl_u64	$d2, $d2, 3;
add_u64	$d0, $d0, $d2;
add_u64	$d1, $d1, 4;
// atomicnoret_st_global_screl_system_b64	[$d0], $d1;
st_arg_u64  $d1, [%ret_r0];

ld_global_u64 $d10, [&signal_Xmalloc];
signalnoret_sub_screl_s64_sig64 $d10, 1;

ret;

};
